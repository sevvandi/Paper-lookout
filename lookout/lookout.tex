% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\author{}
\date{\vspace{-2.5em}}

\begin{document}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Outliers, anomalies and novelties are often interchangeably used to
describe the same concept: data points that are unusual compared to the
rest. The existence of multiple words to describe similar concepts arise
from the growth of outlier detection and applications in multiple
research areas. Indeed, outlier detection is used in diverse
applications ranging from detecting security breaches in the Internet of
Things networks to identifying extreme weather events. Consequently, it
is important to develop robust techniques to detect outliers, which
minimize costly false positives and dangerous false negatives.

This diverse literature can be divided into approaches that use
probability densities to define outliers, and those that use distances
to define outliers. Outlier detection methods that use probability
densities treat outliers as observations that are very unlikely given
the other observations. Outlier detection methods that use distances
treat outliers as observations that lie far from other observations.

In this paper, we take a probability density approach to outlier
detection. We propose a new outlier detection algorithm that we call
\emph{lookout}, which uses leave-one-out kernel density estimates to
identify the most unlikely observations. We address the challenge of
bandwidth selection by using persistent homology --- a concept in
topological data analysis --- and use extreme value theory (EVT) to
identify outliers based on their leave-one-out density estimates.

The main challenge in using kernel density estimates for outlier
detection is the selection of the bandwidth. @Schubert2014 employ kernel
density estimates to detect outliers using \(k\)-nearest neighbor
distances where \(k\) is a user-specified parameter, which determines
bandwidth. @Qin2019 employ kernel density estimates to detect outliers
in streaming data. They too have a radius parameter, which is equivalent
to the bandwidth that needs to be specified by the user. @Tang2017 use
reverse and shared \(k\) nearest neighbors to compute kernel density
estimates and identify outliers. They also have a user defined parameter
\(k\) that denotes the reverse \(k\) nearest neighbors. The oddstream
algorithm {[}@talagala2019anomaly{]} computes kernel density estimates
on a 2-dimensional projection defined by the first two principal
components, and so bandwidths need to be selected. We avoid subjective
user-choice, and the inappropriate use of bandwidths optimized for some
other purpose, by proposing the use of persistent homology as a new tool
for bandwidth selection.

Extreme Value Theory has been gaining popularity in outlier detection
because of its rich, theoretical foundations. @Burridge2006 used EVT to
detect outliers in time series data. @Clifton2014 used Generalized
Pareto Distributions to model the tails in high-dimensional data and
detect outliers. Other recent advances in outlier detection that use EVT
include the stray {[}@stray{]}, oddstream {[}@talagala2019anomaly{]} and
HDoutliers {[}@wilkinson2017visualizing{]} algorithms. Of these three
methods, stray is an enhancement of HDoutliers and both use distances to
detect outliers, while oddstream uses kernel density estimates to detect
outliers in time series data. Our approach is closest to oddstream in
that we also apply EVT to functions of kernel density estimates.
However, we use a different functional, and we avoid the need for an
outlier-free training set.

A brief introduction to persistent homology and EVT is given in Section
\ref{sec:methodology}. In Section \ref{sec:lookout} we introduce the
algorithm \emph{lookout} and the concept of outlier persistence, which
explores the birth and death of outliers with changing bandwidth. We
show examples illustrating the usefulness of outlier persistence and
conduct experiments using synthetic data to evaluate the performance of
lookout in Section \ref{sec:simulations}. Using an extensive data
repository of real datasets, we compare the performance of lookout to
HDoutliers, stray, \textcolor{blue}{KDEOS} {[}@Schubert2014{]}
\textcolor{blue}{and RDOS} {[}@Tang2017{]} in Section
\ref{sec:applications}.

We have produced an R package \texttt{lookout} {[}@lookoutR{]}
containing this algorithm. In addition, all examples in this paper are
available in the supplementary material at
\url{https://github.com/sevvandi/supplementary_material/tree/master/lookout}.

We have used the R packages \texttt{TDAstats} {[}@tdastatsR{]} and
\texttt{ggtda} {[}@ggdta{]} for all TDA and persistent homology
computations and related graphs. We have used the R package \texttt{evd}
{[}@evdR{]} to fit the Generalized Pareto Distribution.

\hypertarget{sec:methodology}{%
\section{Mathematical background}\label{sec:methodology}}

In this section we provide some brief background on three topics that we
will use in our proposed \texttt{lookout} algorithm.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  topological data analysis and persistent homology;
\item
  extreme value theory and the peaks-over-threshold approach; and
\item
  kernel density estimation.
\end{enumerate}

\hypertarget{subsec:tda}{%
\subsection{Topological data analysis and persistent
homology}\label{subsec:tda}}

Topological data analysis is the study of data using topological
constructs. It is about inferring high dimensional structure from low
dimensional representations such as points and assembling discrete
points to construct global structures {[}@ghrist2008barcodes{]}.
Persistent homology is a method in algebraic topology that computes
topological features of a space that persist across multiple scales or
spatial resolutions. These features include connected components,
topological circles and trapped volumes. Features that persist for a
wider range of spatial resolutions represent robust, intrinsic features
of the data while features that sporadically change are perturbations
resulting from noise. Persistent homology has been used in a wide
variety of applications including biology {[}@topaz2015topological{]},
computer graphics {[}@carlsson2008local{]} and engineering
{[}@perea2015sliding{]}. In this section we will give a brief overview
of persistent homology without delving into the mathematical details.
Readers are referred to @ghrist2008barcodes and @Carlsson2009 for an
overview and @wasserman2018topological for a statistical viewpoint on
the subject.

\hypertarget{simplicial-complex}{%
\subsubsection*{Simplicial complex}\label{simplicial-complex}}
\addcontentsline{toc}{subsubsection}{Simplicial complex}

Consider a data cloud representing a collection of points. This set of
points is used to construct a graph where the points are considered
vertices and the edges are determined by the distance between the
points. Given a proximity parameter \(\varepsilon\), two vertices are
connected by an edge if the distance between these two points is less
than or equal to \(\varepsilon\). Starting from this graph, a simplicial
complex --- a space built from simple pieces --- is constructed. A
simplicial complex is a finite set of \(k\)-simplices, where \(k\)
denotes the dimension; for example, a point is a \(0\)-simplex, an edge
a \(1\)-simplex, a triangle a \(2\)-simplex, and a tetrahedron a
\(3\)-simplex. Suppose \(S\) denotes a simplicial complex that includes
a \(k\)-simplex \(\gamma\). Then all non-empty subsets of
\(\beta \subset \gamma\) are also included in \(S\). For example, if
\(S\) contains a triangle \(pqr\), then the edges \(pq\), \(qr\) and
\(rs\), and the vertices \(p\), \(q\) and \(r\), are also in \(S\).

The \emph{Vietoris-Rips} complex and the \emph{Cech} complex are two
types of \(k\)-simplicial complexes. We will construct a Vietoris-Rips
complex from the data cloud as it is more computationally efficient than
the Cech complex {[}@ghrist2008barcodes{]}. Given a set of points and a
proximity parameter \(\varepsilon > 0\), \(k+1\) points within a
distance of \(\varepsilon\) to each other form a \(k\)-simplex. For
example, consider 5 points \(p\), \(q\), \(r\), \(s\) and \(t\) and
suppose the distance between any two points except \(t\) is less than
\(\varepsilon\). Then we can construct the edges \(pq\), \(pr\), \(ps\),
\(qr\), \(qs\) and \(rs\). From the edges \(pq\), \(qr\) and \(rp\) we
can construct the triangle \(pqr\), from \(pq\), \(qs\) and \(sp\) the
triangle \(pqs\) and so on, because the distance between any two points
\(p\), \(q\), \(r\) and \(s\) is bounded by \(\varepsilon\). By
constructing the \(4\) triangles \(pqr\), \(qrs\), \(rsp\) and \(spq\)
we can construct the tetrahedron \(pqrs\). The vertex \(t\) is not
connected to this \(3\)-simplex because the distance between \(t\) and
the other vertices is greater than \(\varepsilon\). The simplicial
complex resulting from these 5 points consists of the tetrahedron
\(pqrs\) and all the subset \(k\)-simplices and the vertex \(t\). Figure
\ref{fig:tetrahedron} shows this simplicial complex on the left and
another example on the right.

\begin{figure}
\centering
\includegraphics{lookout_files/figure-latex/tetrahedron-1.pdf}
\caption{The figure on the left shows the points \(p\), \(q\), \(r\),
\(s\) and \(t\) with a proximity parameter \(\varepsilon = 0.5\) and the
resulting Rips complex consisting of the tetrahedron \(pqrs\), triangles
\(pqr\), \(qrs\), \(rsp\), \(pqs\), edges \(pq\), \(qr\), \(rs\),
\(sp\), \(qs\), \(pr\) and vertices \(p\), \(q\), \(r\), \(s\) and
\(t\). The figure on the right shows \(8\) points and the resulting Rips
complex with \(\varepsilon=4/3\).}
\end{figure}

\hypertarget{persistent-homology}{%
\subsubsection*{Persistent homology}\label{persistent-homology}}
\addcontentsline{toc}{subsubsection}{Persistent homology}

Given a point cloud of data, the resulting Rips complex depends on the
value of the proximity parameter \(\varepsilon\). As we increase
\(\varepsilon\), topological features such as connected components and
holes appear and disappear. This is the focus of persistent homology.
For example, in Figure~\ref{fig:annulus}, we start with a large number
of connected components (top-left) and as \(\varepsilon\) increases to
\(0.8\) the number of connected components merge and decrease to 1
(bottom-left). Around this value of \(\varepsilon\), a hole appears and
as \(\varepsilon\) increases to \(1.5\), it disappears (bottom-right).
The appearances and disappearances of these topological features are
referred to as births and deaths and are illustrated using a
\emph{barcode} or a \emph{persistence diagram}.

\begin{figure}
\centering
\includegraphics{lookout_files/figure-latex/annulus-1.pdf}
\caption{Rips complexes resulting from different \(\varepsilon\)
values.}
\end{figure}

\begin{figure}
\includegraphics[width=1\linewidth]{lookout_files/figure-latex/barcodeandpersistence-1} \caption{The graph on the left shows the barcode of the point cloud in Figure \ref{fig:annulus} and the graph on the right shows the persistence diagram.}\label{fig:barcodeandpersistence}
\end{figure}

Figure \ref{fig:barcodeandpersistence} shows the barcode and the
persistence diagram of the point cloud shown in Figure
\ref{fig:annulus}. The barcode comprises a set of horizontal line
segments, each denoting a feature that starts at its birth diameter and
ends at its death diameter. These line segments are grouped by their
dimension. The orange lines in Figure \ref{fig:barcodeandpersistence}
denote the zero dimensional holes, which are connected components and
blue lines denote one dimensional holes. The longer blue line which is
born at 0.51 and dies at 1.33, corresponds to the hole at the center of
the point cloud in Figure \ref{fig:annulus}. Such features that continue
for a large range of \(\varepsilon\), represent structural properties of
the data that are of interest to us. The same information is presented
differently in the persistence diagram, where the birth and the death of
each feature is denoted by a point. Points away from the diagonal inform
about the structure of the data while points closer to the diagonal are
perturbations related to noise. In this plot, the triangle near the top
represents the same feature as the long blue line in the left plot.

These considerations lead to a natural question: which \(\varepsilon\)
is most representative of the structure of the data cloud? We will
return to this question later.

\hypertarget{subsec:evt}{%
\subsection{Extreme value theory}\label{subsec:evt}}

Extreme Value Theory is used to model rare, extremal events. It is used
in many industries including hydrology (to study 100-year floods),
finance (to explore extreme risks) and insurance (to mitigate against
losses due to disasters) {[}@Reiss2001{]}. EVT has also been used in
outlier detection {[}@wilkinson2017visualizing;@talagala2019anomaly{]}.
In this section we will give a brief introduction to EVT using the
notation in @coles2001introduction.

Consider \(n\) independent and identically distributed random variables
\(X_1, \dots, X_n\) with a distribution function
\(F(x) = P\{X \leq x\}\). Then the maximum of these \(n\) random
variables is \(M_n = \max \{X_1, \dots, X_n\}\). If \(F\) is known, the
distribution of \(M_n\) is given by {[}@coles2001introduction, p45{]}
\(P\{M_n \leq z \} = \left(F(z)\right)^n\). However, \(F\) is usually
not known in practice. This gap is filled by Extreme Value Theory, which
studies approximate families of models for \(F^n\) so that extremes can
be modeled and uncertainty quantified. The Fisher-Tippet-Gnedenko
Theorem states that under certain conditions, a scaled maximum
\(\frac{M_n - a_n}{b_n}\) can have certain limit distributions.

\begin{theorem}[Fisher-Tippett-Gnedenko]\label{thm:FisherTippett}
    If there exist sequences $\{a_n\}$ and $\{b_n\}$ such that
    $$
        P\left\{ \frac{(M_n - a_n)}{b_n} \leq z \right\} \rightarrow G(z) \quad \text{as} \quad n \to \infty,
    $$
    where $G$ is a non-degenerate distribution function, then $G$ belongs to one of the following families:
    \begin{align}\label{eq:EVT3}
        &\text{Gumbel} :  && G(z) = \exp\left(-\exp \left[- \Big(\frac{z-b}{a}\Big) \right] \right), \quad -\infty < z < \infty , \\
        &\text{Fréchet} : && G(z) =
        \begin{cases}
            0 ,                                                           & z \leq b  , \\
            \exp \left( - \left( \frac{z-b}{a}\right)^{-\alpha} \right) , & z > b    ,
        \end{cases}                     \\
        &\text{Weibull} : && G(z) =
        \begin{cases}
            \exp \left( - \left(- \left[\frac{z-b}{a}\right]\right)^{\alpha} \right) , & z < b  ,    \\
            1 ,                                                                        & z \geq b  ,
        \end{cases}
    \end{align}
    for parameters $a, b$ and $\alpha$ where $a, \alpha >0$.
\end{theorem}

These three families of distributions can be further combined into a
single family by using the following distribution function known as the
Generalized Extreme Value (GEV) distribution,
\begin{equation}\label{eq:EVT4}
    G(z) = \exp\left\{ -\left[ 1 + \xi\Big(\frac{z - \mu}{\sigma} \Big)\right]^{-1/\xi} \right\} ,
\end{equation} where the domain of the function is
\(\{z: 1 + \xi (z - \mu)/\sigma >0 \}\). The location parameter is
\(\mu\in\mathbb{R}\), \(\sigma>0\) is the scale parameter, while
\(\xi\in\mathbb{R}\) is the shape parameter. When \(\xi = 0\) we obtain
a Gumbel distribution with exponentially decaying tails. When
\(\xi < 0\) we get a Weibull distribution with a finite upper end and
when \(\xi > 0\) we get a Fréchet family of distributions with
polynomially decaying tails.

\hypertarget{the-generalized-pareto-distribution-and-the-pot-approach}{%
\subsubsection*{The Generalized Pareto Distribution and the POT
approach}\label{the-generalized-pareto-distribution-and-the-pot-approach}}
\addcontentsline{toc}{subsubsection}{The Generalized Pareto Distribution
and the POT approach}

The Peaks Over Threshold (POT) approach regards extremes as observations
greater than a threshold \(u\). We can write the conditional probability
of extreme events as \begin{equation}\label{eq:POT1}
    P\left \{X > u + y \mid X > u \right \} = \frac{1 - F(u+y)}{1 - F(u)} , \quad y >0  ,
\end{equation} giving us \begin{equation}\label{eq:POT2}
    P\left \{X \leq u + y \mid X > u \right \} = \frac{ F(u+y) - F(u)}{1 - F(u)} , \quad y >0  .
\end{equation} The distribution function \begin{equation}\label{eq:POT3}
    F_u(y) = P\left \{X \leq u + y \mid X > u \right \} ,
\end{equation} describes the \emph{exceedances} above the threshold
\(u\). If \(F\) is known we could compute this probability. However, as
\(F\) is not known in practice we use approximations based on the
Generalized Pareto Distribution {[}@Pickands1975{]}.

\begin{theorem}[Pickands]
  Let $X_1,  X_2, \dots,  X_n$ be a sequence of independent random variables with a common distribution function $F$, and let $M_n = \max \{X_1, \dots, X_n \}$. Suppose $F$ satisfies Theorem \ref{thm:FisherTippett} so that for large $n$, $P\{ M_n \leq z \} \approx G(z)$, where
    $$
    G(z) = \exp\left\{ -\left[ 1 + \xi\Big(\frac{z - \mu}{\sigma} \Big)\right]^{-1/\xi} \right\},
  $$
    for some $\mu, \xi \in \mathbb{R}$ and $\sigma >0$. Then for large enough $u$, the distribution function of $(X-u)$ conditional on $X > u$, is approximately
    \begin{equation}\label{eq:POT4}
        H(y) = 1 - \Big( 1 + \frac{\xi y}{\sigma_u} \Big)^{-1/\xi} ,
    \end{equation}
    where the domain of $H$ is $\{y: y >0 \text{~and~} (1 + \xi y)/\sigma_u >0 \}$, and $\sigma_u = \sigma + \xi(u- \mu)$.
\end{theorem}

The family of distributions defined by equation \eqref{eq:POT4} is
called the \textbf{Generalized Pareto Distribution} (GPD). We note that
the GPD parameters are determined from the associated GEV parameters. In
particular, the shape parameter \(\xi\) is the same in both
distributions.

For a chosen threshold \(u\), the parameters of the GPD can be estimated
by standard maximum likelihood techniques. As an analytical solution
that maximizes the likelihood does not exist, numerical techniques are
used to arrive at an approximate solution. We use the R package
\texttt{evd} to fit a GPD using the POT approach.

\hypertarget{subsec:kde}{%
\subsection{Kernel density estimation}\label{subsec:kde}}

For a sample \(\bm{x}_1, \bm{x}_2, \dots, \bm{x}_n \in \mathbb{R}^p\),
the kernel density estimate is given by \begin{equation}\label{eq:kde6}
  \hat{f}(\bm{x}; \bm{H}) = \frac{1}{n} \sum_{i=1}^n K_{\bm{H}}\left(\bm{x} -\bm{x}_i \right) ,
\end{equation} where \(\bm{H}\) denotes a \(p \times p\) positive
definite bandwidth matrix,
\(K_{\bm{H}}(\bm{z}) = \bm{H}^{-1/2} K(\bm{H}^{-1/2} \bm{z})\), and
\(K\) is a kernel function. For consistency, we scale the kernels so
that \(\int \|\bm{z}^2\| K(\bm{z}) d\bm{z} = 1\). The multivariate
Gaussian kernel is given by \begin{equation}\label{eq:kde7}
  K(\bm{x}) = \frac{1}{(2\pi)^{p/2}} \exp( \| \bm{x} \| ^2/2) ,
\end{equation} and the multivariate scaled Epanechnikov kernel by
\begin{equation}\label{eq:kde8}
  K(\bm{x}) = \frac{p+2}{c_p}(1 - \| \bm{x} \| ^2 / 5)_+  ,
\end{equation} where \(c_p\) is the volume of the unit sphere in
\(\mathbb{R}^p\) and \(u_+ = \max(0,u)\).

We will use the leave-one-out kernel density estimator given by
\begin{equation}\label{eq:kde9}
    \hat{f}_{-j}(\bm{x};\bm{H}) = \frac{1}{n-1}\sum_{i \neq j} K_{\bm{H}}(\bm{x}-\bm{x}_i),
\end{equation} which can be simplified to
\begin{equation}\label{eq:kde9b}
  \hat{f}_{-j}(\bm{x}_j;\bm{H}) = \big[n\hat{f}(\bm{x}_j;\bm{H}) - \bm{H}^{-1/2}K(\bm{0})\big]/(n-1)
\end{equation} when evaluated at the observation omitted.

A major challenge in using kernel density estimates for outlier
detection is selecting the appropriate bandwidth. There is a large body
of literature on kernel density estimation and bandwidth selection
{[}@Scott1994;@Wang2019{]} that focuses on computing density estimates
that represent the data as accurately as possible, where measures of
accuracy have certain asymptotic properties. However, our goal is
somewhat different as we are interested in finding outliers in the data,
rather than finding a good representation for the rest of the data.
Often the usual bandwidth selection methods result in bandwidths that
are too small and can cause the kernel density estimates of the boundary
and near-boundary points to be confused with outliers. In high
dimensions this problem is exacerbated due to the sparsity of the data.
Thus, we need a bandwidth that assists outlier detection. A too small
bandwidth causes everything to be outliers, while too large a bandwidth
will lead to outliers being hidden.

\hypertarget{sec:lookout}{%
\section{Methodology}\label{sec:lookout}}

\hypertarget{subsec:selectingBandwidth}{%
\subsection{Bandwidth selection using
TDA}\label{subsec:selectingBandwidth}}

To select a bandwidth for a kernel density estimate designed for outlier
detection, we propose to use the barcode discussed in Section
\ref{subsec:tda}. First we construct the barcode of the data cloud for
dimension zero using the Vietoris-Rips diameter. From the barcode we
obtain the sequence of death diameters \(\{d_i\}_{i = 1}^N\) for the
connected components. By construction this is an increasing sequence as
seen in Figure \ref{fig:barcodeandpersistence}.

\begin{figure}
\includegraphics[width=1\linewidth]{lookout_files/figure-latex/TDAKNN-1} \caption{Top left: A scatterplot of 1000 observations with most points falling on an annulus and some points near the center. Its TDA barcode on the top right and the violin plot of TDA death diameters and KNN distances at the bottom.}\label{fig:TDAKNN}
\end{figure}

Consider the example shown in Figure @ref(fig:TDAKNN). At the top left,
the data are shown with most points lying on an annulus, and a few
points near the centre. The barcodes for dimension 0 are shown at the
top right, with lengths equal to the death diameters
\(\{d_i\}_{i = 1}^N\). The bottom left panel shows violin plots
comparing the death diameters (denoted by TDA) with \(k\) nearest
neighbor distances for \(k \in \{1, 5, 10\}\). The TDA Rips diameters
fall within the range of the combined KNN distances. Consequently, we
can use TDA distances without having to select the parameter \(k\) for
KNN distances.

The plot on the bottom right in Figure \ref{fig:TDAKNN} shows the
largest 20 Rips diameters (out of the 999 diameters shown in the top
right plot). A vertical dashed line is drawn at diameter 0.468, the
second largest Rips diameter. As the diameter increases from 0.468 till
1.041, which is the maximum diameter, the number of connected components
stay the same. For this point cloud, \((0.468, 1.041)\) is the largest
diameter range for which the number of components stay the same. Thus,
it signifies a global structural property of the point cloud. We want to
take this structure into account when selecting the bandwidth.
Therefore, we choose a diameter that gives rise to persisting features,
which in our case are connected components. We consider the Rips
diameter intervals \((d_i, d_{i+1})\) for all \(i\), and find the
largest interval by computing successive differences
\(\Delta d_i = d_{i+1} - d_i\), for \(i \in \{1, \cdots, N-1 \}\). We
choose the Rips diameter \(d_i\) corresponding to the maximum
\(\Delta d_i\): \begin{equation}\label{eq:deathrad2}
    d_* = d_{i_*} \qquad \text{where ~} i_*  = \argmax \{ \Delta d_i \}_{i=1}^{N-1}.
\end{equation} Then the bandwidth matrix is given by
\begin{equation}\label{eq:bandwidth}
    \bm{H} = d_*^{2/p} \bm{I},
\end{equation} so that \begin{align}\label{eq:kde11}
    \| \bm{H}^{-1/2}(\bm{x} - \bm{x}_i) \|^2
      %& = (\bm{x} - \bm{x}_i)^T (\bm{H}^{-1/2})^T \bm{H}^{-1/2} (\bm{x} - \bm{x}_i)  , \notag \\
      %& = (\bm{x} - \bm{x}_i)^T \bm{H}^{-1} (\bm{x} - \bm{x}_i)  , \notag                                \\
      %& = \frac{1}{h^p}(\bm{x} - \bm{x}_i)^T (\bm{x} - \bm{x}_i) , \notag                                \\
      & = \frac{1}{d_*^2}\|\bm{x} - \bm{x}_i \|^2  .
\end{align} This ensures that points within a distance of \(d_*\)
contribute to the kernel density estimate of \(\bm{x}\), resulting in
the following leave-one-out kernel density estimate
\begin{align}\label{eq:kde12}
    \hat{f}_{-j}(\bm{x}_j;\bm{H})
      %& = \left[ n \hat{f}(\bm{x}_j;\bm{H}) - \bm{H}^{-1/2}K(\bm{0})\right]/(n-1)  \\
      & = \big[ n \hat{f}(\bm{x}_j;\bm{H}) - d_*^{-1} K(\bm{0})\big]/(n-1).
\end{align}

\hypertarget{algorithm-lookout}{%
\subsection{\texorpdfstring{Algorithm
\emph{lookout}}{Algorithm lookout}}\label{algorithm-lookout}}

Now we have all the building blocks necessary to describe the algorithm
\emph{lookout}. Consider an \(N \times p\) data matrix \(\bm{X}\) with
\(N\) observations in \(\mathbb{R}^p\). It is customary in outlier
detection to scale the data so that all variables contribute equally to
outlier detection. We normalize the data using Min-Max normalization,
which scales each attribute to \([0, 1]\) and has been shown to be
effective compared to other normalization techniques
{[}@kandanaarachchi2018normalization{]}. To accommodate datasets that do
not need to be normalized, we make normalization a user option.

We compute the kernel density estimates defined by \eqref{eq:kde6} and
the leave-one-out kernel density estimates defined in \eqref{eq:kde12},
with the bandwidth matrix \eqref{eq:bandwidth}, and the scaled
Epanechnikov kernel \eqref{eq:kde8}.

Denote the kernel density estimate of \(\bm{x}_i\) by \(y_i\) and the
leave-one-out kde of \(\bm{x}_i\) (by leaving out \(\bm{x}_i\)) by
\(y_{-i}\). Then we fit a Generalized Pareto Distribution to
\(-\log(y_i)\) using the POT approach discussed in Section
\ref{subsec:evt}. We use the \(90^{\text{th}}\) percentile as the
threshold for the POT approach as recommended by @bommier2014peaks.
Using the fitted GPD parameters, \(\mu\), \(\sigma\) and \(\xi\), we
declare points with
\(P\left(-\log(y_{-i})|\mu, \sigma,\xi \right) < \alpha\) to be
outliers. We summarize these steps in Algorithm \ref{algo:lookout}.

\DontPrintSemicolon
\begin{algorithm}\fontsize{11}{16}\selectfont
    \SetKwInOut{Input}{input~~~}
    \SetKwInOut{Output}{output}
    \Input{~ The data matrix $\bm{X}$, parameters $\alpha$ and \textit{unitize}.}
    \Output{~ The outliers, the GPD probabilities of all points, GPD parameters and bandwidth}
    If \textit{unitize = TRUE}, then normalize the data so that each column is scaled to $[0,1]$.\\
    Construct the persistence homology barcode of the data. \\
    Find $d_*$ as in equation \eqref{eq:deathrad2}. \\
    Using $\bm{H} = (d_*)^{2/p}\bm{I}$, compute kernel density estimates \eqref{eq:kde6} and leave-one-out kernel density estimates \eqref{eq:kde12} using the scaled Epanechnikov kernel \eqref{eq:kde8}. \\
    Denote the kde of $\bm{x}_i$ by $y_i$ and leave-one-out kde by $y_{-i}$.\\
    Using the POT approach, fit a GPD to $\{-\log(y_i)\}_{i=1}^N$ and estimate $\mu, \sigma$ and $\xi$. \\
    Using the GPD estimates $\hat\mu$, $\hat\sigma$ and $\hat\xi$, find the probability of the leave-one-out kde values $\{-\log(y_{-i})\}_{i=1}^N$, i.e., $P\left(-\log(y_{-i})|\mu, \sigma, \xi \right)$ for all $i$. \\
    If $P\left(-\log(y_{-i})|\mu, \sigma, \xi \right) < \alpha$, then declare $\bm{x}_i$ as an outlier.
    \caption{\itshape lookout.}
    \label{algo:lookout}
\end{algorithm}

The output probability of lookout is the GPD probability of the points,
so that low probabilities indicate likely outliers and high
probabilities indicate normal points. Note that the scaling factor of
the kernel \(K(\bm{x})\) does not affect the GPD parameters as it is
just an offset after taking logs of \(y_i\) or \(y_i\).

The algorithm lookout has only 2 inputs, \(\alpha\) and \emph{unitize}.
The parameter \(\alpha\) determines the threshold for outlier detection
and \emph{unitize} gives the user an option to normalize the data. We
set \(\alpha= 0.05\) and \emph{unitize} = \texttt{TRUE} as default
parameter values. We use the Epanechnikov kernel in lookout due to ease
of computation. However, any kernel can be incorporated as long as the
variances are matched.

\hypertarget{subsec:persistence}{%
\subsection{Outlier persistence}\label{subsec:persistence}}

Lookout identifies outliers by selecting an appropriate bandwidth using
TDA. If the bandwidth is changed, will the original outliers be still
identified as outliers by lookout? We explore this question by varying
bandwidth values in lookout. Similar work is discussed by @Minnotte1993,
who introduce the Mode Tree, which tracks the modes of the kernel
density estimates with changing bandwidth values. Another related idea
is the SiZer map {[}@Chaudhuri1999{]}, a graphical device that studies
features of curves for varying bandwidth values.

\begin{figure}
\includegraphics[width=1\linewidth]{lookout_files/figure-latex/outlierpersistence-1} \caption{Outliers at the center of the annulus in the left plot showing the outlier labels 1001--1005. The outlier persistence diagram on the right with the y-axis denoting the labels. The dashed line shows the lookout bandwidth $d_*$.}\label{fig:outlierpersistence}
\end{figure}

If a point is identified as an outlier by the lookout algorithm for a
range of bandwidth values, then it increases the validity of that point
as an outlier. Consider an annulus with some points in the middle as
shown in the left plot of Figure \ref{fig:outlierpersistence}. The plot
on the right, which is similar to a barcode, shows the outliers
identified by lookout for different bandwidth values. Each horizontal
line segment shows the range of Rips diameter values that has identified
each point as an outlier. With some abuse of notation, we label the
\(x\) axis ``bandwidth'', even though it actually represents the Rips
diameter \(d_* = h^{p/2}\), where the bandwidth matrix
\(\bm{H} = h\bm{I}\). This is motivated from equation \eqref{eq:kde11},
as we only consider points \(\bm{x}_i\) within a distance \(d_*\) of
every point \(\bm{x}\) when computing kernel density estimates using the
Epanechnikov kernel. In this plot, the y-axis corresponds to the point
index. We call this plot \emph{the outlier persistence diagram}
signifying the link to topological data analysis.

In the example in Figure \ref{fig:outlierpersistence}, we see that the
points 1001--1005 are identified as outliers in the outlier persistence
diagram for a large range of bandwidth values. The Rips diameter \(d_*\)
selected by lookout is shown by a vertical dashed line. Many points are
identified as outliers for small bandwidth values but do not continue to
be outliers for long; these outliers are born at small bandwidth values
and die out after a relatively short increase in bandwidth. Some points
are never identified as outliers, even at small bandwidths.

The outlier persistence diagram is a tool to observe the persistence of
outliers with changing bandwidth values. We vary the bandwidth values
while keeping the GPD parameters fixed at the values obtained using
\(d_*\) as in equation \eqref{eq:deathrad2}. The death diameter sequence
\(d_i\) is used to construct the set of bandwidth values used in this
plot. We use \(\ell\) bandwidth values starting from the
\(\beta^{\text{th}}\) percentile of sequence \(\{d_i\}\) ending at
\(\sqrt{5} \times \max_i{d_i}\). The parameters \(\ell\) and \(\beta\)
are user-defined with default values \(\ell = 20\) and \(\beta= 90\).
Increasing \(\ell\) gives better granularity but increases the
computational burden. As the death diameters are tightly packed, the
default value of \(90^{\text{th}}\) percentile gives a small enough
starting bandwidth and \(\sqrt{5} \max_i{d_i}\) gives a large ending
bandwidth. We summarize these steps in Algorithm \ref{algo:persistence}.

\DontPrintSemicolon
\begin{algorithm}\fontsize{11}{16}\selectfont
    \SetKwInOut{Input}{input~~~}
    \SetKwInOut{Output}{output}
    \Input{~ The data matrix $\bm{X}$, parameters $\alpha$, \textit{unitize} and bandwidth range parameters $\ell$ and $\beta$}
    \Output{~ An $N \times \ell $ binary matrix $\bm{Y}$ where $N$ denotes the number of observations and $\ell$ denotes the number of bandwidth values with $y_{ik} = 1$ if the $i^{\text{th}}$ observation is identified as an outlier for bandwidth index ${k}$. }
    Initialize matrix $\bm{Y}$ to zero. \\
    Run Algorithm \ref{algo:lookout} to determine the death diameter sequence $\{d_i\}$ and the GPD parameters $\mu_0$, $\sigma_0$ and $\xi_0$. \\
    Construct an equidistant bandwidth sequence of length $\ell$ starting from the $\beta^{\text{th}}$ percentile of $\{d_i\}$ to $\sqrt{5} \max_i d_i$. Call the bandwidth sequence $\{b_k\}_{k=1}^{\ell}$. \\
    \For{$k$ from $1$ to $\ell$}{
    Using $h = (b_k)^{2/p}$ and $\bm{H} = h\bm{I}$ compute kernel density estimates and leave-one-out kernel density estimates using the scaled Epanechnikov kernel. \\
    Denote the kde of $\bm{x}_i$ by $y_i$ and leave-one-out kde by $y_{-i}$.\\
    Using the GPD parameters $\mu_0, \sigma_0$ and $\xi_0$ find the GPD probability of the leave-one-out kde values $\{-\log(y_{-i})\}_{i=1}^N$, i.e., $P\left(-\log(y_{-i})|\mu_0, \sigma_0, \xi_0 \right)$ for all $i$. \\
    If $P\left(-\log(y_{-i})|\mu_0, \sigma_0, \xi_0 \right) < \alpha$, then declare $\bm{x}_i$ as an outlier and let $y_{ik} = 1$.
    }
    \caption{\itshape outlier persistence for fixed $\alpha$.}
    \label{algo:persistence}
\end{algorithm}

Next, we extend this notion of persistence to include significance
levels \(\alpha \in \{0.01, 0.02, \dots, 0.1 \}\).

We define the \emph{strength} of an outlier based on its significance
level. Let \(\bm{x}_j\) be an outlier identified at significance level
\(\alpha\), where \(\alpha\) is the smallest significance level for
which \(\bm{x}_j\) is identified as an outlier. Then
\begin{equation}\label{eq:strengthpersistence}
    \text{strength} (\bm{x}_j) = \frac{(0.11 - \alpha)_+}{0.01}
\end{equation} Thus, if a point is identified as an outlier with a
significance level \(\alpha = 0.01\), then it has strength 10, and an
outlier with \(\alpha = 0.1\) has strength 1. To compute persistence
over significance levels, the only modification that needs to be done to
Algorithm \ref{algo:persistence} is to fix \(\alpha = 0.1\) and to
record the minimum significance level if
\(P\left(-\log(y_{-i})|\mu_0, \sigma_0, \xi_0 \right) < 0.1\). Then we
can use equation \eqref{eq:strengthpersistence} to compute its strength.

\begin{figure}
\centering
\includegraphics{lookout_files/figure-latex/outlierpersistence2-1.pdf}
\caption{Outlier persistence over different bandwidth values and their
strengths. The dashed line corresponds to the lookout bandwidth
\(d_*\).}
\end{figure}

Figure \ref{fig:outlierpersistence2} shows the persistence of outliers
over different bandwidth values and significance levels for the dataset
in Figure \ref{fig:outlierpersistence}. We see that points 1001--1005
are identified as outliers with high strength even for large bandwidths.
This gives a comprehensive representation of outliers as it encapsulates
the bandwidth as well as the strength (which corresponds to
significance).

\hypertarget{sec:timeseriesoutiers}{%
\subsection{\texorpdfstring{\textcolor{blue}{Time series outliers}}{}}\label{sec:timeseriesoutiers}}

\textcolor{blue}{Following} @Burridge2006
\textcolor{blue}{we extend lookout to a time series setting. They consider a time series with $k$ additive outliers defined as }

\[ z_t = y_t + \sum_{j=1}^k \mathcal{I}_{\tau_j} x_{\tau_j} \, , \]
\textcolor{blue}{where} \[ \mathcal{I}_{\tau_j} =  \begin{cases} 
      1\, , & t = \tau_j \\
      0\, ,  & t \neq \tau_j
   \end{cases} \, ,
\] \textcolor{blue}{and } \[ y_t = \phi y_{t-1} + u_t \, , \]
\textcolor{blue}{with $\{u_t\}$ an autoregressive moving average process. They consider the case when  $\phi = 1$  and estimate $x_{\tau_j}$ by }
\[ \hat{x}_{\tau_j} = \frac{1}{2}\left(\Delta z_{\tau_j} - \Delta z_{\tau_j +1}  \right)  = \frac{1}{2}\left(\Delta y_{\tau_j} - \Delta y_{\tau_j +1}  + 2 x_{\tau_j}\right) \, .\]
\textcolor{blue}{ From the spacings of $\hat{x}_{t}$ they find outliers using  Algorithm 1 in their paper. Due to successive differences each outlier in $\{z_{t}\}$ can give rise to 3 consecutive outliers in $\{\hat{x}_{t}\}$. We extend lookout to a time series setting by using it on $\{\hat{x}_{t}\}$. To mitigate for successive outliers, we select the observation with the highest strength or the lowest probability from each set of consecutive outliers. }

\hypertarget{sec:simulations}{%
\section{Experiments with synthetic data}\label{sec:simulations}}

In this section we first explore outlier persistence using simple
examples. Then we conduct experiments using synthetic data comparing
lookout to HDoutliers {[}@wilkinson2017visualizing{]}, stray
{[}@stray{]}, \textcolor{blue}{KDEOS} {[}@Schubert2014{]} and
\textcolor{blue}{RDOS} {[}@Tang2017{]}. Both HDoutliers and stray use
extreme value theory to detect outliers; KDEOS and RDOS use kernel
density estimates.

\hypertarget{sec:PersistenceExamples}{%
\subsection{Outlier persistence
examples}\label{sec:PersistenceExamples}}

\begin{figure}
\includegraphics[width=1\linewidth]{lookout_files/figure-latex/persistenceEx-1} \caption{Examples of outlier persistence and strength. The left column shows the data with the outliers identified by lookout with $\alpha = 0.1$ colored by its strength. The right column shows the outlier persistence diagrams for these examples. The dashed line corresponds to the lookout bandwidth.}\label{fig:persistenceEx}
\end{figure}

Figure \ref{fig:persistenceEx} shows five examples in \(\mathbb{R}^2\),
each showing the data, outliers identified by lookout, their strengths
and the corresponding outlier persistence diagram. In each example, the
outliers are placed at the end of the synthetic dataset, i.e.~their
observation indices are the highest. The dashed lines in the outlier
persistence diagram indicate the bandwidth chosen by lookout. The top
left plot shows normally distributed data with 5 outliers in the top
right corner, which are identified by lookout with high strength. Upon
close inspection we see two further points, both approximately at
\((-2.6, 1.8)\) and slightly detached from the main group of points,
also being identified by lookout as outliers with low strength. The
lookout strength of each of these points is less than 2, and so they
would only be identified as outliers when \(\alpha > 0.09\). The
corresponding persistence diagram shows high strength outlier
persistence for the points at the top right hand corner. Similarly, from
other graphs in column 1, we see that outliers far from high density
regions are identified by lookout with high strength and points outside
high density regions, but not so far away are identified with low
strength. For the example in row 3, lookout has identified a cluster of
outliers approximately at \((6,6)\) with high strength as well as
individual points slightly away from high density regions with low
strength. The example in row 5 has single outliers rather than clusters,
which are again identified by lookout. For the last example in the
bottom row, lookout has identified a cluster of outliers at
\((-0.6, 0.35)\) with low strength and the remaining outliers with high
strength. From the persistence diagrams we see that many points are
identified as outliers for low bandwidth values, but only a small number
of outliers persist until the bandwidth is large. In addition, some
outliers persist at low strength values. For example, the persistence
diagram in row 5 has outliers around index 500 with low strength
persisting until large bandwidth values.

From the examples in Figure \ref{fig:persistenceEx} we see that lookout
selects a bandwidth appropriate for outlier detection that is neither
too small nor too large. In addition, lookout identifies outliers
correctly. The outlier persistence diagram gives a snapshot of the
outliers for varying bandwidths and significance levels increasing our
understanding of the dataset and its outliers.

\hypertarget{sec:SyntheticComparison}{%
\subsection{Comparison Study}\label{sec:SyntheticComparison}}

In this section we conduct three experiments with synthetic data. Each
experiment considers two data distributions; the main distribution and a
handful of outliers which are distributed differently. There are several
iterations to each experiment. The iterations serve as a measure of the
degree of outlyingness of the small sample. The outliers start off with
the main distribution and slowly move out of the main distribution with
each iteration. Consequently, in the initial iterations the points in
the outlying distribution are not actual outliers as they are similar to
the points in the main distribution, while in the later iterations they
are quite different from the main distribution. We repeat each iteration
10 times to account for the randomness of the data generation process.

We compare the results of lookout with \textcolor{blue}{4} other
algorithms; HDoutliers {[}@wilkinson2017visualizing{]}, stray
{[}@stray{]}, \textcolor{blue}{KDEOS} {[}@Schubert2014{]}
\textcolor{blue}{ and RDOS} {[}@Tang2017{]}.
\textcolor{blue}{ Both HDoutliers and stray use extreme value theory for outlier detection while both KDEOS and RDOS use kernel density estimates. HDoutliers and stray identify outliers, that is outliers are assigned a binary label as output of these algorithms. KDEOS and RDOS do not identify outliers. Instead, they give an anomaly score which can be used to rank outliers. As the algorithms exhibit differences in their output mechanisms, we employ different methods to compare the performance of these algorithms. To compare the performance of lookout with HDoutliers and stray we use the identified outliers. We use F-measure and the geometric mean of sensitivity and specificity denoted by Gmean, which we discuss below to compare these 3 algorithms. To compare lookout with KDEOS and RDOS we use the outlier scores and utilize the area under the Receiver Operator Characteristic curve denoted by AUC. Furthermore, the evaluation metrics F-measure, Gmean and AUC are suited for imbalanced datasets. For KDEOS and RDOS we use the default parameters. For lookout, HDoutliers and stray we use $\alpha = 0.05$. }
The F-measure is defined as \begin{equation}\label{eq:fmeasure}
    \text{F-measure} = 2\frac{\text{precision} \times \text{recall}}{\left( \text{precision} + \text{recall} \right) }  ,
\end{equation} where \begin{equation}\label{eq:pr}
    \text{precision} = \frac{ \textit{tp} }{\textit{tp} + \textit{fp}}  , \qquad \text{and} \qquad \text{recall} = \frac{\textit{tp}}{\textit{tp} + \textit{fn}}  ,
\end{equation} where \emph{tp}, \emph{fp} and \emph{fn} denote true
positives (predicted = true, actual = true), false positives (predicted
= true, actual = false) and false negatives (predicted = false, actual
=true) respectively. The F-measure is undefined when both precision and
recall are zero, which occurs when the true positives \emph{tp} are
zero. This happens when the outlier detection algorithm does not
identify any correct outliers. We assign zero to the F-measure in such
instances.

Sensitivity and specificity are similar evaluation metrics more
frequently used in a medical diagnosis context:
\begin{equation}\label{eq:ss}
    \text{sensitivity} = \frac{ \textit{tp} }{\textit{tp} + \textit{fn}}  , \qquad \text{and} \qquad \text{specificity} = \frac{\textit{tn}}{\textit{tn} + \textit{fp}}  ,
\end{equation} and \begin{equation}\label{eq:gmean}
    \text{Gmean} = \sqrt{ \text{sensitivity} \times \text{specificity}}  ,
\end{equation} where \emph{tn} denotes the true negatives (predicted =
false, actual = false). In fact, sensitivity and recall are two
different terms denoting the same quantity of interest.

\textcolor{blue}{For all 3 experiments we compute the time taken for each algorithm on a Microsoft Surface Pro 3 laptop, with Intel(R) Core(TM) i5-4300U, 2.50GHz processor. We report the time taken as additional metrics.}

\hypertarget{experiment-1}{%
\subsubsection*{Experiment 1}\label{experiment-1}}
\addcontentsline{toc}{subsubsection}{Experiment 1}

For this experiment we consider two normally distributed samples in
\(\mathbb{R}^6\); one large and one small starting at the same location
with the small sample slowly moving out in each iteration. The set of
points belonging to the small sample are considered outliers. We
consider 400 points in the bigger sample and 5 in the outlying sample,
placed at indices 401--405. The points in the larger sample are
distributed in each dimension as \(\mathcal{N}(0, 1)\). The outliers
differ from the normal points in only one dimension; i.e.~they are
distributed as \(\mathcal{N}\left(2 + (i-1)/2, 0.2 \right)\), where
\(i\) denotes the iteration. In the first iteration the outliers are
distributed as \(\mathcal{N}\left(2, 0.2 \right)\) in the first
dimension and in the tenth iteration they are distributed as
\(\mathcal{N}\left(6.5, 0.2 \right)\). Each iteration is repeated 10
times to account for randomness.

\begin{figure}
\includegraphics[width=1\linewidth]{lookout_files/figure-latex/ComparisonEx1-1} \caption{Experiment 1 with outliers moving out from the normal distribution in $\mathbb{R}^6$. Graph on the top left shows the first two dimensions in the last iteration and last repetition.  The graph on the top right shows the performance comparison between lookout, HDoutliers, stray, kdeos and rdos using AUC, Fmeasure and Gmean over 10 repetitions. Fmeasure and Gmean are used for algorithms that identify outliers, while AUC is used for algorithms that rank outliers. In this experiment, stray and HDoutliers gave identical results. The graph on the bottom left shows the time taken for these 5 algorithms. The graph on the bottom right shows the corresponding outlier persistence plot.}\label{fig:ComparisonEx1}
\end{figure}

The top left graph of Figure \ref{fig:ComparisonEx1} shows the first two
dimensions of this experimental dataset in its last iteration and
repetition with outliers identified by lookout shown in different
colors.
\textcolor{blue}{ The top right graph shows the performance comparison of lookout, HDoutliers, stray, KDEOS and RDOS. As lookout, HDoutliers and stray identify outliers, we have used the Fmeasure and Gmean to compare their performance. As KDEOS and RDOS rank outliers instead of identifying them, we have used the AUC -- the area under the ROC curve -- to compare their performance. We see that for each iteration lookout surpasses HDoutliers and stray significantly. Lookout gives much better performance than KDEOS, and from the fourth iteration onwards lookout surpasses RDOS. The graph on the bottom left shows the time taken for each algorithm. We see that RDOS takes a much longer time than the other algorithms. HDoutliers and stray are the fastest, followed by lookout. The graph on the bottom right shows the outlier persistence plot for this data with the dashed line denoting the lookout bandwidth.}

\hypertarget{experiment-2}{%
\subsubsection*{Experiment 2}\label{experiment-2}}
\addcontentsline{toc}{subsubsection}{Experiment 2}

For this experiment we consider an annulus in \(\mathbb{R}^2\) with
outlying points moving into the center with each iteration. We consider
\(800\) points in the annulus with \(5\) outlying points. The outliers
are normally distributed and have a smaller standard deviation compared
to the other points. The mean of the outliers in the \(i^{\text{th}}\)
iteration is \(\left( 5 - (i-1) /2, 0 \right)\), so that the outliers
start at the right of the annulus with mean \((5,0)\) and move in with
each iteration, ending with mean \((0,0)\). We repeat each iteration 10
times.

\begin{figure}
\includegraphics[width=1\linewidth]{lookout_files/figure-latex/ComparisonEx2-1} \caption{Experiment 2 with outliers moving into the center of the annulus in $\mathbb{R}^2$. Graph on the top left shows the points from the last iteration and repetition. The graph on the top right shows the performance comparison between lookout, HDoutliers, stray, kdeos and rdos using AUC, Fmeasure and Gmean over 10 repetitions. The graph on the bottom left shows the time taken for these 5 algorithms. The graph on the bottom right shows the corresponding outlier persistence plot.}\label{fig:ComparisonEx2}
\end{figure}

The graph at the top left of Figure \ref{fig:ComparisonEx2} shows the
points in the final iteration and repetition.
\textcolor{blue}{ The graph at the top right shows the performance comparison using AUC, F-measure and Gmean. We see that lookout performs better than the other algorithms across different metrics. The graph on the bottom left shows the time taken for each algorithm. RDOS takes much longer compared to others, while HDoutliers and stray are the fastest followed by lookout.}
The graph on the bottom right shows the outlier persistence for the
final iteration and repetition. The outliers are placed at indices
801--805 and we see that they are not identified as outliers for small
bandwidth values because the outliers are clustered together. This shows
the importance of bandwidth selection for outlier detection when using
kernel density estimates. The bandwidth selected by lookout is shown as
a dashed line.

\hypertarget{experiment-3}{%
\subsubsection*{Experiment 3}\label{experiment-3}}
\addcontentsline{toc}{subsubsection}{Experiment 3}

For this experiment we consider a unit cube in \(\mathbb{R}^{20}\) with
500 points, of which 499 points are uniformly distributed in each
direction and the remaining point is an outlier. The outlier moves
towards the point \(\left( 0.9, 0.9, \dots, 0.9 \right)\) with each
iteration. For the \(i^{\text{th}}\) iteration the first \(i\)
coordinates of the outlier are each equal to \(0.9\) and the remaining
coordinates are uniformly distributed in \((0,1)\). The index of the
outlier is 500. Each iteration is repeated 10 times with different
randomizations.

\begin{figure}
\includegraphics[width=1\linewidth]{lookout_files/figure-latex/ComparisonEx3-1} \caption{Experiment 3 with an outlier moving to $(0.9, 0.9, \dots)$. Graph on the top shows the performance comparison between lookout, HDoutliers, stray, KDEOS and RDOS using AUC, Fmeasure and Gmean over 10 repetitions. The graph on the bottom left shows the time taken for these 5 algorithms. The graph on the bottom right shows the corresponding outlier persistence plot.}\label{fig:ComparisonEx3}
\end{figure}

\textcolor{blue}{The top graph in Figure \ref{fig:ComparisonEx3} shows the performance comparison between the different algorithms. We see that RDOS performs better than lookout in the initial 8 iterations of this experiment. After the $8^{\text{th}}$ iteration lookout performs better than RDOS. We also see that lookout performs significantly better than stray and HDoutliers. The graph on the bottom left shows the time taken for the 5 algorithms. Stray is the fastest followed by HDoutliers. The slowest algorithm is RDOS. The graph in the bottom right }
shows the outlier persistence for the last iteration and repetition. The
dashed line shows the bandwidth chosen by lookout.

\hypertarget{section}{%
\subsubsection*{\texorpdfstring{\textcolor{blue}{Comparison of false positives }}{}}\label{section}}
\addcontentsline{toc}{subsubsection}{\textcolor{blue}{Comparison of false positives }}

\textcolor{blue}{
A low false positive rate is an attractive feature for an outlier detection method. High false positives diminish user confidence with the risk of abandonment of such systems; consider a home security system with constant alarms. To get an understanding of the false positives, we redo experiments 1-3 without any outliers and compare the specificity ($tn/(tn+fp)$) of HDoutliers, stray and lookout. We repeat each experiment 10 times. As KDEOS and RDOS do not identify outliers, we do not include them in this comparison. High values of specificity is preferred as low values indicate more false positives. Table \ref{tab:zerooutliers} shows the specificity comparison results. Stray has the highest, average specificity of 1 with zero standard deviation;  HDoutliers has a similar specificity for all three experiments. We see that the average specificity of lookout is greater than $0.99$ with low standard deviation values. Thus lookout's specificity is comparable to HDoutliers and stray, demonstrating that its false positive rate is low. }

\begin{table}

\caption{\label{tab:zerooutliers}Specificity for lookout, HDoutliers and stray algorithms when no outliers are present.}
\centering
\begin{tabular}[t]{rrrrrrr}
\toprule
\multicolumn{1}{c}{\textbf{}} & \multicolumn{2}{c}{\textbf{lookout}} & \multicolumn{2}{c}{\textbf{stray}} & \multicolumn{2}{c}{\textbf{HDoutliers}} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5} \cmidrule(l{3pt}r{3pt}){6-7}
Experiment & mean & sd & mean & sd & mean & sd\\
\midrule
\textcolor{blue}{1} & \textcolor{blue}{0.9941} & \textcolor{blue}{0.0031} & \textcolor{blue}{1} & \textcolor{blue}{0} & \textcolor{blue}{1.0000} & \textcolor{blue}{0e+00}\\
\textcolor{blue}{2} & \textcolor{blue}{0.9924} & \textcolor{blue}{0.0024} & \textcolor{blue}{1} & \textcolor{blue}{0} & \textcolor{blue}{0.9999} & \textcolor{blue}{4e-04}\\
\textcolor{blue}{3} & \textcolor{blue}{0.9942} & \textcolor{blue}{0.0032} & \textcolor{blue}{1} & \textcolor{blue}{0} & \textcolor{blue}{1.0000} & \textcolor{blue}{0e+00}\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{section-1}{%
\subsection*{\texorpdfstring{\textcolor{blue}{Evaluating time series outliers }}{}}\label{section-1}}
\addcontentsline{toc}{subsection}{\textcolor{blue}{Evaluating time series outliers }}

\textcolor{blue}{
In this section we compare the time series implementation of lookout with the outlier detection method proposed by }
@Burridge2006,
\textcolor{blue}{which we denote by BT. For this example we simulate an ARIMA$(1, 1, 0)$ time series with the autoregressive parameter taking values $0.5, 0.6, 0.7, 0.8$ and $0.9$. We add an outlier to each simulated time series and repeat the simulation 10 times for each parameter value to account for randomness. Table \ref{tab:timeseriesdatviz} gives the results of lookout and BT using Gmean and Fmeasure. We see that lookout gives better performance than BT for all parameter values. 
}

\begin{table}

\caption{\label{tab:timeseriesdatviz}Time series outliers performance comparison}
\centering
\begin{tabular}[t]{rrrrrrrrr}
\toprule
\multicolumn{1}{c}{\textbf{}} & \multicolumn{4}{c}{\textbf{Gmean}} & \multicolumn{4}{c}{\textbf{FMeasure}} \\
\cmidrule(l{3pt}r{3pt}){2-5} \cmidrule(l{3pt}r{3pt}){6-9}
\multicolumn{1}{c}{\textbf{}} & \multicolumn{2}{c}{\textbf{lookout}} & \multicolumn{2}{c}{\textbf{BT}} & \multicolumn{2}{c}{\textbf{lookout}} & \multicolumn{2}{c}{\textbf{BT}} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5} \cmidrule(l{3pt}r{3pt}){6-7} \cmidrule(l{3pt}r{3pt}){8-9}
AR & mean & sd & mean & sd & mean & sd & mean & sd\\
\midrule
\textcolor{blue}{0.5} & \textcolor{blue}{1.0} & \textcolor{blue}{0.000} & \textcolor{blue}{0.700} & \textcolor{blue}{0.483} & \textcolor{blue}{1.0} & \textcolor{blue}{0.000} & \textcolor{blue}{0.667} & \textcolor{blue}{0.471}\\
\textcolor{blue}{0.6} & \textcolor{blue}{1.0} & \textcolor{blue}{0.000} & \textcolor{blue}{0.899} & \textcolor{blue}{0.316} & \textcolor{blue}{1.0} & \textcolor{blue}{0.000} & \textcolor{blue}{0.750} & \textcolor{blue}{0.326}\\
\textcolor{blue}{0.7} & \textcolor{blue}{0.9} & \textcolor{blue}{0.316} & \textcolor{blue}{0.500} & \textcolor{blue}{0.527} & \textcolor{blue}{0.9} & \textcolor{blue}{0.316} & \textcolor{blue}{0.500} & \textcolor{blue}{0.527}\\
\textcolor{blue}{0.8} & \textcolor{blue}{1.0} & \textcolor{blue}{0.000} & \textcolor{blue}{0.599} & \textcolor{blue}{0.515} & \textcolor{blue}{1.0} & \textcolor{blue}{0.000} & \textcolor{blue}{0.467} & \textcolor{blue}{0.443}\\
\textcolor{blue}{0.9} & \textcolor{blue}{0.9} & \textcolor{blue}{0.316} & \textcolor{blue}{0.500} & \textcolor{blue}{0.527} & \textcolor{blue}{0.9} & \textcolor{blue}{0.316} & \textcolor{blue}{0.467} & \textcolor{blue}{0.502}\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{sec:applications}{%
\section{Results on two data repositories}\label{sec:applications}}

For this section we use
\textcolor{blue}{two data repositories: the ODDS data repository}
{[}@datasetsODDS{]} \textcolor{blue}{and the repository at} @datasets.
\textcolor{blue}{Using the ODDS repository, we will look at the individual performance of 12 well known datasets to these 5 outlier detection methods. As}
@datasets
\textcolor{blue}{ has more than $12000$ outlier detection datasets, we will evaluate the performance of these methods on the repository and report summary statistics because the number of datasets is large. }

\hypertarget{odds-data-repository}{%
\subsection{ODDS data repository}\label{odds-data-repository}}

\textcolor{blue}{Table \ref{tab:ODDResults} shows the results of lookout, HDoutliers, stray, KDEOS and RDOS on  12 ODDS datasets. As previously, we have used F-measure and Gmean to compare the performance of lookout,  HDoutliers and stray, and AUC to compare lookout, KDEOS and RDOS. We see that for datasets glass, optdigits and wine, none of the 3 algorithms lookout, HDoutliers and stray identify any outliers. As such, the Gmean and F-measure are zero for these three datasets.  For the other 9 datasets lookout outperforms stray and HDoutliers.  }

\textcolor{blue}{The AUC values in Table \ref{tab:ODDResults} show that lookout gives the best performance for 8 of the 12 datasets. For the speech dataset lookout is tied with RDOS. RDOS gives the best performance for 3 datasets. Two datasets give errors for KDEOS.
}

\begin{table}

\caption{\label{tab:ODDResults}Performance evaluation of 12 datasets in the ODDS repository.}
\centering
\begin{tabular}[t]{lrrrrrrrrr}
\toprule
\multicolumn{1}{c}{\textbf{}} & \multicolumn{3}{c}{\textbf{Gmean}} & \multicolumn{3}{c}{\textbf{FMeasure}} & \multicolumn{3}{c}{\textbf{AUC}} \\
\cmidrule(l{3pt}r{3pt}){2-4} \cmidrule(l{3pt}r{3pt}){5-7} \cmidrule(l{3pt}r{3pt}){8-10}
Filename & lookout & stray & HDoutliers & lookout & stray & HDoutliers & lookout & KDEOS & RDOS\\
\midrule
\textcolor{blue}{cardio} & \textcolor{blue}{0.32} & \textcolor{blue}{0.00} & \textcolor{blue}{0.08} & \textcolor{blue}{0.19} & \textcolor{blue}{0.00} & \textcolor{blue}{0.01} & \textcolor{blue}{0.80} & \textcolor{blue}{0.48} & \textcolor{blue}{0.55}\\
\textcolor{blue}{glass} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.51} & \textcolor{blue}{0.44} & \textcolor{blue}{0.43}\\
\textcolor{blue}{letter} & \textcolor{blue}{0.10} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.02} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.50} & \textcolor{blue}{0.58} & \textcolor{blue}{0.91}\\
\textcolor{blue}{lympho} & \textcolor{blue}{0.58} & \textcolor{blue}{0.41} & \textcolor{blue}{0.00} & \textcolor{blue}{0.50} & \textcolor{blue}{0.29} & \textcolor{blue}{0.00} & \textcolor{blue}{0.99} & \textcolor{blue}{0.77} & \textcolor{blue}{0.98}\\
\textcolor{blue}{musk} & \textcolor{blue}{0.69} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.64} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{1.00} & \textcolor{blue}{NA} & \textcolor{blue}{0.22}\\
\addlinespace
\textcolor{blue}{optdigits} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.46} & \textcolor{blue}{0.59} & \textcolor{blue}{0.75}\\
\textcolor{blue}{satimage-2} & \textcolor{blue}{0.96} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.94} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.98} & \textcolor{blue}{0.37} & \textcolor{blue}{0.76}\\
\textcolor{blue}{speech} & \textcolor{blue}{0.13} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.02} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.52} & \textcolor{blue}{NA} & \textcolor{blue}{0.52}\\
\textcolor{blue}{thyroid} & \textcolor{blue}{0.31} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.13} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.70} & \textcolor{blue}{0.60} & \textcolor{blue}{0.57}\\
\textcolor{blue}{vowels} & \textcolor{blue}{0.24} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.09} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.62} & \textcolor{blue}{0.64} & \textcolor{blue}{0.83}\\
\addlinespace
\textcolor{blue}{wbc} & \textcolor{blue}{0.44} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.31} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.84} & \textcolor{blue}{0.48} & \textcolor{blue}{0.72}\\
\textcolor{blue}{wine} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.00} & \textcolor{blue}{0.65} & \textcolor{blue}{0.50} & \textcolor{blue}{0.61}\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{a-large-data-repository}{%
\subsection{A large data repository}\label{a-large-data-repository}}

The repository at @datasets has more than \(12000\) outlier detection
datasets that were prepared from classification datasets. Dataset
preparation involves downsampling the minority class in classification
datasets, converting the categorical variables to numerical and
accounting for missing values, all of which is detailed in
@normalizationoutliers.

\textcolor{blue}{ We evaluate the performance of these 5 outlier detection methods on this data repository. While lookout, HDoutliers and stray gave valid output for all 12000+ datasets, RDOS and KDEOS gave errors for 5745 datasets. As such, we only compare lookout, stray and HDoutliers on this data repository.}

\begin{figure}
\includegraphics[width=1\linewidth]{lookout_files/figure-latex/lvplots-1} \caption{Letter value plots of performance differences between 1. lookout and HDoutliers, 2. lookout and stray using Gmean and Fmeasure.}\label{fig:lvplots}
\end{figure}

Figure \ref{fig:lvplots} shows the letter-value plots {[}@lvplots{]} of
performance differences between (a) lookout and HDoutliers and (b)
lookout and stray, using Gmean and Fmeasure after removing the entries
that have zero values for all three algorithms. Letter-value plots
enhance traditional box-plots by including more detailed information
making them suitable for large datasets. The letter-value plots in
Figure \ref{fig:lvplots} are area adjusted, that is, the area of each
box represents the number of points/datasets in it. The median is
represented by a white line and each black box represents a fourth of
the population denoted by F in the legend. The next box represents a
eighth denoted by an E and each successive box represents half of the
previous one.

\begin{table}

\caption{\label{tab:table}Summary statistics for comparing lookout with the HDoutliers and stray algorithms.}
\centering
\begin{tabular}[t]{lll}
\toprule
Statistic & lookout - HDoutliers & lookout - stray\\
\midrule
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Fmeasure}}\\
\hspace{1em}Median & 0.1307 & 0.1385\\
\hspace{1em}Mean & 0.0487 & 0.0838\\
\hspace{1em}95\% Confidence Interval & (0.0385,0.0589) & (0.0743,0.0932)\\
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Gmean}}\\
\hspace{1em}Median & 0.0405 & 0.0406\\
\hspace{1em}Mean & 0.0711 & 0.0768\\
\hspace{1em}95\% Confidence Interval & (0.0662,0.0760) & (0.0721,0.0815)\\
\bottomrule
\end{tabular}
\end{table}

From the graphs in Figure \ref{fig:lvplots} we see that the total area
of the letter-value plots above zero is larger than the area below zero.
This signifies that more datasets have positive Gmean and F-measure
values for lookout - HDoutliers and lookout - stray performance values
compared to the negative values. This is confirmed by the results in
Table \ref{tab:table}. We see that for both Gmean and F-measure, the
median, mean and the 95\% confidence interval from Student's t-tests are
away from zero. In fact, the Gmean has median values 0.1307 and 0.1385
for lookout - HDoutliers and lookout - stray respectively. Similarly,
the corresponding F-measure values are 0.0405 and 0.0406. Given that
both Gmean and F-measure are bounded by 1, this shows that lookout gives
better performance than HDoutliers or stray.

\hypertarget{sec:conclusions}{%
\section{Conclusions}\label{sec:conclusions}}

Lookout uses leave-one-out kernel density estimates and EVT to detect
outliers. Outlier detection methods that use kernel density estimates
generally employ a user-defined parameter to construct the bandwidth.
Selecting a bandwidth for outlier detection is different from selecting
a bandwidth for general data representation, because the goal is to make
outliers have lower density estimates compared to the non-outliers. In
addition, it is a challenge to select an appropriate bandwidth in high
dimensions. To make outliers have lower density estimates compared to
the rest, a reasonably large bandwidth needs to be chosen. We introduced
an algorithm called \emph{lookout} that uses persistent homology to
select the bandwidth.

We compared the performance of lookout
\textcolor{blue}{with 4 outlier detection algorithms, 2 of which use EVT to detect outliers and the others use KDE. These algorithms are HDoutliers, stray, KDEOS and RDOS.}
Our results on experimental data and on
\textcolor{blue}{two data repositories} showed that lookout achieves
better performance.

We also introduced the concept of \emph{outlier persistence}, exploring
the birth and death of outliers with changing bandwidth and significance
values. Outlier persistence gives a bigger picture, taking a step back
from fixed parameter values. It explores the bandwidth and significance
parameters and highlights the outliers that persist over a range of
bandwidth values and their significance levels. We suggest that it is a
useful measure that increases our understanding of outliers.

\hypertarget{sec:suppmat}{%
\section{Supplementary materials}\label{sec:suppmat}}

The R package \texttt{lookout} is available at
\url{https://github.com/sevvandi/lookout}. The outlier detection data
repository used in Section \ref{sec:applications} is available at
@datasets. The programming scripts used in Sections
\ref{sec:simulations} and \ref{sec:applications} are available at
\url{https://github.com/sevvandi/supplementary_material/tree/master/lookout}.

\end{document}
